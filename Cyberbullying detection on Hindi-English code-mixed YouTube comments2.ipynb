{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CYBERBULLYING DETECTION IN HINDI-ENGLISH CODE-MIXED YOUTUBE COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## July 2018 Project\n",
    "### Author: Kshiti Ballal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "* Developer key\n",
    "* Video id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authentication\n",
    "\n",
    "from googleapiclient.discovery import build \n",
    "from googleapiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "import pafy\n",
    "\n",
    "# File handling\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Text cleaning\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import nltk.classify.util as util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder as BCF\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google API Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of youtube video : \n",
      "zT-Gd2djRV0\n"
     ]
    }
   ],
   "source": [
    "with open(\"client_secret.txt\", 'r', encoding=\"utf-8\") as fp:\n",
    "        client_secret=str(fp.readline())\n",
    "\n",
    "DEVELOPER_KEY = client_secret\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "pafy.set_api_key(\"client_secret\")\n",
    "\n",
    "def add_data(comments):\n",
    "\n",
    "    count = 0\n",
    "    with open(\"COMMENTS.csv\", 'w', encoding=\"utf-8\") as fp:\n",
    "        for comment in comments:\n",
    "            count += 1\n",
    "            print(comment, file=fp)\n",
    "            sys.stdout.write('Downloaded %d comments\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "    print('\\nDone!')\n",
    "\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "videoId = input(\"ID of youtube video : \\n\")\n",
    "url = \"https://www.youtube.com/watch?v=\" + videoId\n",
    "\n",
    "#Request for comm of the Video\n",
    "video = pafy.new(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Youtube comments for video: https://www.youtube.com/watch?v=zT-Gd2djRV0\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Downloaded 4494 comments\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from youtube.commentThreads()\n",
    "\n",
    "results = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    maxResults=100,\n",
    "    videoId=videoId,\n",
    "    textFormat=\"plainText\",\n",
    ").execute()\n",
    "\n",
    "totalResults = int(results[\"pageInfo\"][\"totalResults\"])\n",
    "count = 0\n",
    "nextPageToken = ''\n",
    "comments = []\n",
    "further = True\n",
    "first = True\n",
    "print('Downloading Youtube comments for video:', url)\n",
    "while further:\n",
    "    halt = False\n",
    "    print('.')\n",
    "    if first == False:\n",
    "        try:\n",
    "            results = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                maxResults=100,\n",
    "                videoId=videoId,\n",
    "                textFormat=\"plainText\",\n",
    "                pageToken=nextPageToken,\n",
    "            ).execute()\n",
    "            totalResults = int(results[\"pageInfo\"][\"totalResults\"])\n",
    "        except HttpError as e:\n",
    "            print(\"An HTTP error %d occurred:\\n%s\" % (e.resp.status, e.content))\n",
    "            halt = True\n",
    "            \n",
    "    if halt == False:\n",
    "        count += totalResults\n",
    "        for item in results[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "            author = comment[\"snippet\"][\"authorDisplayName\"]\n",
    "            text = comment[\"snippet\"][\"textDisplay\"].replace(\"\\n\", \"\")\n",
    "            comments.append(author + \"| \" + text)\n",
    "        if totalResults < 100:\n",
    "            further = False\n",
    "            first = False\n",
    "        else:\n",
    "            further = True\n",
    "            first = False\n",
    "            try:\n",
    "                nextPageToken = results[\"nextPageToken\"]\n",
    "            except KeyError as e:\n",
    "                print(\"An KeyError error occurred: %s\" % (e))\n",
    "                further = False\n",
    "                \n",
    "add_data(comments) # call the function to save comments to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 634: expected 2 fields, saw 3\\nSkipping line 3553: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Users</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Niyati Khatr</td>\n",
       "      <td>If a girl doesn't regret doing porn after rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ritu Biswas</td>\n",
       "      <td>Bloody asshole when you're sitting for an int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful life</td>\n",
       "      <td>Why did the interviwer always focus on her ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Panshul Jetwani</td>\n",
       "      <td>You're fucking judgemental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yusra</td>\n",
       "      <td>Sunny you Are a Really strong Woman ‚ù§ I wish ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Users                                           Comments\n",
       "0     Niyati Khatr   If a girl doesn't regret doing porn after rea...\n",
       "1      Ritu Biswas   Bloody asshole when you're sitting for an int...\n",
       "2   Beautiful life   Why did the interviwer always focus on her ne...\n",
       "3  Panshul Jetwani                         You're fucking judgemental\n",
       "4            Yusra   Sunny you Are a Really strong Woman ‚ù§ I wish ..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Conversion of users and comments data to Dataframe\n",
    "\n",
    "documents = pd.read_csv('COMMENTS.csv',delimiter='|',error_bad_lines=False,header=None)\n",
    "documents.columns=(\"Users\", \"Comments\")\n",
    "documents.drop_duplicates(subset=\"Comments\",keep='first', inplace=True)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments extracted: 4292\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of comments extracted:\",documents[\"Comments\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words used per comment: 26\n",
      "Maximum number of words used per comment: 505\n",
      "Minimum number of words used per comment: 2\n"
     ]
    }
   ],
   "source": [
    "documents['word_count'] = documents['Comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "documents[['Comments','word_count']].sort_values(\"word_count\",ascending=False).head()\n",
    "print(\"Average number of words used per comment:\",round(documents[\"word_count\"].mean()))\n",
    "print(\"Maximum number of words used per comment:\",round(documents[\"word_count\"].max()))\n",
    "print(\"Minimum number of words used per comment:\",round(documents[\"word_count\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Missing values \n",
    "\n",
    "documents.isnull().values.any()\n",
    "# No row or comment is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most frequent words\n",
    "\n",
    "#toklist=[]\n",
    "#for item in documents[\"Comments\"]:\n",
    "#    tokens = word_tokenize(str(item))\n",
    "#    toklist=toklist+tokens\n",
    "#    wordfreqdist = nltk.FreqDist(toklist)\n",
    "#print(str(wordfreqdist.most_common(20))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing emojis and special symbols\n",
    "\n",
    "def emojiremoval(fp,item):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                    u\"\\U00002702-\\U000027B0\"  # other emoticons\n",
    "                    u\"\\U0001F923\" # rolf emoji\n",
    "                    u\"\\U000024C2-\\U0001F251\" \"]+\", flags=re.UNICODE)\n",
    "    fp.write(\"\".join(emoji_pattern.sub(r'', item))+\"\\n\") # no emoji\n",
    "      \n",
    "doc=documents['Comments']\n",
    "with open(\"COMMENTSA.csv\", 'w', encoding=\"utf-8\") as fp:\n",
    "    for item in doc:\n",
    "        emojiremoval(fp,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If a girl doesn't regret doing porn after rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloody asshole when you're sitting for an int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why did the interviwer always focus on her ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You're fucking judgemental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunny you Are a Really strong Woman  I wish Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments\n",
       "0   If a girl doesn't regret doing porn after rea...\n",
       "1   Bloody asshole when you're sitting for an int...\n",
       "2   Why did the interviwer always focus on her ne...\n",
       "3                         You're fucking judgemental\n",
       "4   Sunny you Are a Really strong Woman  I wish Y..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents2 = pd.read_csv('COMMENTSA.csv',delimiter='\\n',header=None)\n",
    "documents2.rename(columns={0: 'Comments'}, inplace=True)\n",
    "documents2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "def normalizing(fp,item):\n",
    "    \n",
    "            # tokenize the sentences\n",
    "            tokens = word_tokenize(item)\n",
    "        \n",
    "             # convert to lower case\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "          \n",
    "            # remove punctuation from each word\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            tokens = [w.translate(table) for w in tokens]\n",
    "           \n",
    "            # remove stopwords from each sentence\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [w for w in tokens if not w in stop_words]\n",
    "               \n",
    "            fp.write(\" \".join(tokens)+\"\\n\")\n",
    "\n",
    "doc2 = documents2[\"Comments\"]\n",
    "with open(\"COMMENTSA.csv\", 'w', encoding=\"utf-8\") as fp:\n",
    "          for item in doc2:\n",
    "            normalizing(fp,item)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl nt regret porn reaching fame heights coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bloody asshole sitting interview asking questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interviwer always focus negative points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fucking judgemental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunny really strong woman wish slapped front e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments\n",
       "0  girl nt regret porn reaching fame heights coul...\n",
       "1  bloody asshole sitting interview asking questi...\n",
       "2            interviwer always focus negative points\n",
       "3                                fucking judgemental\n",
       "4  sunny really strong woman wish slapped front e..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents2 = pd.read_csv('COMMENTSA.csv',delimiter='\\n',header=None)\n",
    "documents2.rename(columns={0: 'Comments'}, inplace=True)\n",
    "documents2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>14049.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399</th>\n",
       "      <td>sunny</td>\n",
       "      <td>1810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>interview</td>\n",
       "      <td>716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7886</th>\n",
       "      <td>u</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>questions</td>\n",
       "      <td>581.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>interviewer</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>respect</td>\n",
       "      <td>546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>nt</td>\n",
       "      <td>536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>like</td>\n",
       "      <td>535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>leone</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>bhupendra</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>chaubey</td>\n",
       "      <td>404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5862</th>\n",
       "      <td>porn</td>\n",
       "      <td>402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>people</td>\n",
       "      <td>344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>person</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>guy</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>way</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>asshole</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>one</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>man</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comments       tf\n",
       "0                  14049.0\n",
       "7399        sunny   1810.0\n",
       "3991    interview    716.0\n",
       "7886            u    697.0\n",
       "6173    questions    581.0\n",
       "3995  interviewer    550.0\n",
       "6444      respect    546.0\n",
       "5371           nt    536.0\n",
       "4631         like    535.0\n",
       "4579        leone    526.0\n",
       "1004    bhupendra    416.0\n",
       "1456      chaubey    404.0\n",
       "5862         porn    402.0\n",
       "5669       people    344.0\n",
       "5698       person    342.0\n",
       "3314          guy    340.0\n",
       "8179          way    322.0\n",
       "593       asshole    300.0\n",
       "5454          one    297.0\n",
       "4833          man    294.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent words through term frequency score\n",
    "\n",
    "tf1 = (documents2['Comments']).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['Comments','tf']\n",
    "tf1.sort_values(\"tf\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(sentifile,item):\n",
    "        sentiment = 0\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        Commentsblob = TextBlob(item)\n",
    "        sentiment = Commentsblob.sentiment.polarity\n",
    "        sentifile.write(item + '|' + str(sentiment) + \"\\n\")\n",
    "        sys.stdout.flush()\n",
    "        if sentiment >= 0:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1  \n",
    "            \n",
    "doc2 = documents2[\"Comments\"]\n",
    "with open('COMMENTSA1.csv', 'w',encoding=\"utf-8\") as sentifile:\n",
    "    for item in doc2:\n",
    "        sentiment(sentifile,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl nt regret porn reaching fame heights coul...</td>\n",
       "      <td>0.233482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bloody asshole sitting interview asking questi...</td>\n",
       "      <td>-0.153571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interviwer always focus negative points</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fucking judgemental</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunny really strong woman wish slapped front e...</td>\n",
       "      <td>-0.183333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Comments  Sentiments\n",
       "0  girl nt regret porn reaching fame heights coul...    0.233482\n",
       "1  bloody asshole sitting interview asking questi...   -0.153571\n",
       "2            interviwer always focus negative points   -0.300000\n",
       "3                                fucking judgemental   -0.600000\n",
       "4  sunny really strong woman wish slapped front e...   -0.183333"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents3 = pd.read_csv('COMMENTSA1.csv',delimiter='|',error_bad_lines=False,header=None)\n",
    "documents3.rename(columns={0: 'Comments',1:'Sentiments'}, inplace=True)\n",
    "documents3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments with less than 0.35 sentiment score:  3690\n",
      "Number of comments with greater than 0.35 sentiment score:  598\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of comments with less than 0.35 sentiment score: \", len(documents3.loc[documents3['Sentiments'] < 0.35]))\n",
    "print(\"Number of comments with greater than 0.35 sentiment score: \", len(documents3.loc[documents3['Sentiments'] >= 0.35]))\n",
    "neg= documents3.loc[documents3['Sentiments'] < 0.35].sort_values(by = \"Sentiments\")\n",
    "pos=documents3.loc[documents3['Sentiments'] >= 0.35].sort_values(by = \"Sentiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>worst interviewer worst chaubey</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>worst interviewer  dislikes proof     gtfo ibn...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>dear bhupender chaubey ever asked questions po...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>bhupendra chaubey worst performance</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>man disgusting        cnn shame</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  Sentiments\n",
       "2060                    worst interviewer worst chaubey        -1.0\n",
       "2091  worst interviewer  dislikes proof     gtfo ibn...        -1.0\n",
       "3113  dear bhupender chaubey ever asked questions po...        -1.0\n",
       "3117               bhupendra chaubey worst performance         -1.0\n",
       "2034                man disgusting        cnn shame            -1.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>ibn key take away reporter interviewer even cl...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>cool leaone  lluv u</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>sunny leone sweet xoxo</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>real man chooses honor  love  respect adore wo...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>hats sunny ya  handled patientlyif bollywood a...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  Sentiments\n",
       "2877  ibn key take away reporter interviewer even cl...        0.35\n",
       "399                                 cool leaone  lluv u        0.35\n",
       "4036                             sunny leone sweet xoxo        0.35\n",
       "3198  real man chooses honor  love  respect adore wo...        0.35\n",
       "1395  hats sunny ya  handled patientlyif bollywood a...        0.35"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"positive.txt\", 'w', encoding=\"utf-8\") as fp:\n",
    "    for item in pos[\"Comments\"]:\n",
    "        fp.write(item + \"\\n\")\n",
    "        \n",
    "with open(\"negative.txt\", 'w', encoding=\"utf-8\") as fp:\n",
    "    for item in neg[\"Comments\"]:\n",
    "        fp.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorization of comments\n",
    "### Extracting the keywords from the list of offensive words text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hurtful comments\n",
    "def category1():\n",
    "\n",
    "    keywords = set()\n",
    "    with open(\"Hurtful.txt\", encoding=\"utf-8\") as list_file1:\n",
    "        for line in list_file1:\n",
    "            if line.strip():\n",
    "                keywords.add(line.strip())\n",
    "    \n",
    "    with open(\"COMMENTSA.csv\", encoding=\"utf-8\") as master_file:\n",
    "        with open(\"HurtfulCom.txt\", 'w', encoding=\"utf-8\") as search_results1:\n",
    "            for line in master_file:\n",
    "                if set(line.split()[:-1]) & keywords:\n",
    "                    search_results1.write(' 1' + '\\t' + line)  # Category 1\n",
    "category1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obscene comments\n",
    "def category2():  \n",
    "    \n",
    "    keywords = set()\n",
    "    with open(\"Obscene.txt\", encoding=\"utf-8\") as list_file2:\n",
    "        for line in list_file2:\n",
    "            if line.strip():\n",
    "                keywords.add(line.strip())\n",
    "    \n",
    "    with open(\"COMMENTSA.csv\", encoding=\"utf-8\") as master_file:\n",
    "        with open(\"ObsceneCom.txt\", 'w', encoding=\"utf-8\") as search_results2:\n",
    "            for line in master_file:\n",
    "                if set(line.split()[:-1]) & keywords:\n",
    "                    search_results2.write(' 2' + '\\t' + line)  # Category 2\n",
    "category2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insulting comments\n",
    "def category3():  \n",
    "    \n",
    "    keywords = set()\n",
    "    with open(\"Insults.txt\", encoding=\"utf-8\") as list_file3:\n",
    "        for line in list_file3:\n",
    "            if line.strip():\n",
    "                keywords.add(line.strip())\n",
    "    \n",
    "    with open(\"COMMENTSA.csv\", encoding=\"utf-8\") as master_file:\n",
    "        with open(\"InsultCom.txt\", 'w', encoding=\"utf-8\") as search_results3:\n",
    "            for line in master_file:\n",
    "                if set(line.split()[:-1]) & keywords:\n",
    "                    search_results3.write(' 3' + '\\t' + line)  # Category 3\n",
    "category3()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Racist comments\n",
    "def category4():  \n",
    "    \n",
    "    keywords = set()\n",
    "    with open(\"Racist.txt\", encoding='latin1') as list_file4:\n",
    "        for line in list_file4:\n",
    "            if line.strip():\n",
    "                keywords.add(line.strip())\n",
    "    \n",
    "    with open(\"COMMENTSA.csv\", encoding=\"utf-8\") as master_file:\n",
    "        with open(\"RacistCom.txt\", 'w', encoding=\"utf-8\") as search_results4:\n",
    "            for line in master_file:\n",
    "                if set(line.split()[:-1]) & keywords:\n",
    "                    search_results4.write(' 4' + '\\t' + line)  # Category 4\n",
    "category4()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining all the files\n",
    "\n",
    "filenames = ['HurtfulCom.txt', 'ObsceneCom.txt', 'InsultCom.txt', 'RacistCom.txt']\n",
    "with open('BullyingCom.txt', 'w', encoding='utf=8') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname,encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ugly  inside n outside  sense die one shows  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>really yaaar u great patient sunny  frustrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hypocrates everywhere  ppl might curse fact pp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>asking questions giving chance speak  hate typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sunny cool dumb questions   hats u ur patience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                           Comments\n",
       "0         1  ugly  inside n outside  sense die one shows  s...\n",
       "1         1  really yaaar u great patient sunny  frustrated...\n",
       "2         1  hypocrates everywhere  ppl might curse fact pp...\n",
       "3         1  asking questions giving chance speak  hate typ...\n",
       "4         1     sunny cool dumb questions   hats u ur patience"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents2 = pd.read_csv('BullyingCom.txt',delimiter='\\t',header=None)\n",
    "documents2.rename(columns={0: 'Category',1:'Comments'}, inplace=True)\n",
    "documents2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MODEL BUILDING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(words):\n",
    "    temp = str(word_tokenize(words))\n",
    "    print(temp)\n",
    "\n",
    "    words = [temp[0]]\n",
    "    for i in range(1, len(temp)):\n",
    "        if(temp[i] != temp[i-1]):\n",
    "            words.append(temp[i])\n",
    "\n",
    "    scoreF = BigramAssocMeasures.chi_sq\n",
    "    bigrams = BCF.from_words(words).nbest(scoreF, 150)\n",
    "\n",
    "    return dict([word,True] for word in itertools.chain(words, bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pos_sen = open(\"positive.txt\", 'r', encoding = 'utf-8').read()\n",
    "neg_sen = open(\"negative.txt\", 'r', encoding = 'utf-8').read()\n",
    "\n",
    "prev = [(features(words), 'positive') for words in pos_sen.split('\\n')]\n",
    "nrev = [(features(words), 'negative') for words in neg_sen.split('\\n')]\n",
    "\n",
    "ncutoff = int(len(nrev)*3/4)\n",
    "pcutoff = int(len(prev)*3/4)\n",
    "train_set = nrev[:ncutoff] + prev[:pcutoff] \n",
    "test_set = nrev[ncutoff:] + prev[pcutoff:]\n",
    "\n",
    "#test_classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "test_classifier = DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "\n",
    "#SAVE IN FILE TO AVOID TRAIINING THE DATA AGAIN\n",
    "save_doc = open(\"classifier.pickle\", 'wb')\n",
    "pickle.dump(test_classifier, save_doc)\n",
    "save_doc.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the classifier pickle file\n",
    "\n",
    "fl = open('classifier.pickle','rb')\n",
    "classifier = pickle.load(fl)\n",
    "fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  83.78378378378379\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy is : \", util.accuracy(test_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurtful comments count:  462\n",
      "Obscene comments count:  1658\n",
      "Insulting comments count:  949\n",
      "Racist comments count:  20\n"
     ]
    }
   ],
   "source": [
    "feat1 = []\n",
    "feat2 = []\n",
    "feat3 = []\n",
    "feat4 = []\n",
    "\n",
    "f1,f2,f3,f4=0,0,0,0\n",
    "\n",
    "with open('BullyingCom.txt', 'r', encoding=\"utf=8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        all_feats = dict(word_feats(row[0].split()).items())\n",
    "        if int(row[0]) == 1:\n",
    "            f1 += 1\n",
    "            feat1.append((all_feats, 'Hurtful'))\n",
    "        if int(row[0]) == 2:\n",
    "            f2 += 1\n",
    "            feat2.append((all_feats, 'Obscene'))\n",
    "        if int(row[0]) == 3:\n",
    "            f3+= 1\n",
    "            feat3.append((all_feats, 'Insulting'))\n",
    "        if int(row[0]) == 4:\n",
    "            f4+= 1\n",
    "            feat4.append((all_feats, 'Racist'))\n",
    "print (\"Hurtful comments count: \", f1)\n",
    "print (\"Obscene comments count: \",f2)\n",
    "print (\"Insulting comments count: \",f3)\n",
    "print(\"Racist comments count: \",f4)\n",
    "\n",
    "train = feat1 + feat2 + feat3 + feat4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects = ('HURTFUL', 'OBSCENE', 'INSULTS', 'RACIST')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [f1,f2,f3,f4]\n",
    "plt.bar(y_pos, performance,align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Comment categorization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.3461915658712\n",
      "99.3461915658712\n",
      "99.3461915658712\n",
      "99.3461915658712\n",
      "99.3461915658712\n",
      "99.3461915658712\n",
      "99.3461915658712\n",
      "100.0\n",
      "100.0\n",
      "99.3461915658712\n",
      "These are the accuracies for our comment categorization classifier.\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for x in range(10):\n",
    "    random.shuffle(train)\n",
    "    cutoff = int(len(train) * 0.01)\n",
    "    trainfeats = train[:cutoff]\n",
    "    testfeats = train[cutoff:]\n",
    " \n",
    "    #classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    classifier = DecisionTreeClassifier.train(trainfeats)\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    accuracies.append(accuracy)\n",
    "    print ((accuracy * 100))\n",
    "print(\"These are the accuracies for our comment categorization classifier.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User comment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a comment: Hey! What's up,doc? That person is a complete Idiot!üòõ üòúüòù\n"
     ]
    }
   ],
   "source": [
    "usercomment=input(\"Enter a comment: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hey! What's up,doc? That person is a complete Idiot! \\n\"]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji removal\n",
    "with open(\"USERCOMMENT.csv\", 'w', encoding=\"utf-8\") as fp:\n",
    "    emojiremoval(fp,usercomment) \n",
    "\n",
    "with open(\"USERCOMMENT.csv\", 'r', encoding=\"utf-8\") as fp:\n",
    "    uline=fp.readlines()\n",
    "uline      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  hey   doc  person complete idiot  n  \\n']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning\n",
    "\n",
    "with open(\"USERCOMMENT.csv\", 'w+', encoding=\"utf-8\") as fp:\n",
    "    normalizing(fp,str(uline)) \n",
    "\n",
    "with open(\"USERCOMMENT.csv\", 'r', encoding=\"utf-8\") as fp:\n",
    "    uline=fp.readlines()\n",
    "uline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['  hey   doc  person complete idiot  n  \\\\n']|-0.35000000000000003\\n\"]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "with open(\"USERCOMMENT.csv\", 'w', encoding=\"utf-8\") as fp:\n",
    "    sentiment(fp,str(uline))\n",
    "\n",
    "with open(\"USERCOMMENT.csv\", 'r', encoding=\"utf-8\") as fp:\n",
    "    uline=fp.readlines()\n",
    "    \n",
    "uline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative comment\n"
     ]
    }
   ],
   "source": [
    "# Check if comment entered has negative or postive sentiment\n",
    "\n",
    "for word in uline:\n",
    "    if '-' in word:\n",
    "        print(\"negative comment\")\n",
    "    else:\n",
    "        print(\"positive comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', \"'\", 'hey', 'doc', 'person', 'complete', 'idiot', 'n', '\\\\n', \"'\", ']', '|-0.35000000000000003']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hey', 'doc', 'person', 'complete', 'idiot']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the words of length greater than 3 as tokens\n",
    "\n",
    "tokenlist=[]\n",
    "for i in uline:\n",
    "    tokens = word_tokenize(i)\n",
    "    print(tokens)\n",
    "for i in tokens:\n",
    "    if len(i)>=3 and i.isalpha():\n",
    "        tokenlist.append(i)\n",
    "tokenlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of badwords\n",
    "\n",
    "i=0\n",
    "badwords=[]\n",
    "filenames = ['Hurtful.txt', 'Obscene.txt', 'Insults.txt', 'Racist.txt']\n",
    "while i!=4:\n",
    "    with open(filenames[i]) as infile:\n",
    "        for line in infile:\n",
    "            badwords.append(line)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your comment contains bullying words!\n"
     ]
    }
   ],
   "source": [
    "# The bullying or non-bullying comment classifier\n",
    "\n",
    "flag=0\n",
    "for t in tokenlist:\n",
    "    for i in badwords:\n",
    "        if t in i:\n",
    "            flag=1\n",
    "       \n",
    "if flag==1:\n",
    "    print(\"Your comment contains bullying words!\")\n",
    "else:\n",
    "    print(\"Your comment is clean and ready to be posted!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
